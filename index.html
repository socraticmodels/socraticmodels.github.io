<!DOCTYPE HTML>
<html>
    <head>
        <title>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="flickity.min.css">
        <script src="flickity.pkgd.min.js"></script>
        <link rel="stylesheet" href="style.css" />
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header"></header>
            <!-- style="padding-bottom:1em" -->
            <div id="profile">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc">
                    <div id="profile-name">Socratic Models</div>
                    <div id="profile-subname">Composing Zero-Shot Multimodal Reasoning with Language</div>
                    <!-- <div id="profile-authors">
                        Andy Zeng&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Adrian Wong&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Stefan Welker&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Krzysztof Choromanski&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Federico Tombari<br>
                        Aveek Purohit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Michael Ryoo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Vikas Sindhwani&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Johnny Lee&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Vincent Vanhoucke&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Pete Florence<br>Google</div> -->
                </div>
                <div id="profile-icons-wrapper">
                    <img src="images/internet-venn.png" style="height: 0px; margin: 0px; padding: 0px;"> <!-- thumbnail -->
                    <div class="profile-icon"><a href="https://arxiv.org/abs/2204.00598"><img src="images/paper-icon.png"></a><p>Paper</p></div>
                    <div class="profile-icon"><a href="#code"><img src="images/github.png"></a><p>Code</p></div>
                </div>
                <div style="clear: both;"></div>
            </div>
<!-- 
            <div class="section abstract">
            <p>
            <video width="70%" playsinline="" muted="" autoplay="" loop="">
                <source src="images/sm_1_venn.mp4" type="video/mp4">
            </video>
            </p>
            </div> -->

            
            <div class="section abstract">
                <h1>Abstract</h1>
                <!-- <img src="images/internet-venn.png" style="height: auto; width: 500px"> -->
                <video width="445px" playsinline="" muted="" autoplay="" loop="" style="float: right; margin-right: 30px; padding-left: 10px; padding-bottom: 10px; padding-top: 7px">
                    <source src="images/sm_1_venn.mp4" type="video/mp4">
                </video>
                <p>Large pretrained (e.g., “foundation”) models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains.  In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs):  a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.</p>
                
            </div>

            <div class="section method">
                <h1><b>Overview:</b> Prompt Engineering Multimodal Applications</h1>
                <p>Socratic Models (SMs) is a framework in which multiple large pretrained models may be composed through language (via prompting) without requiring training, to perform new downstream multimodal tasks. This offers an alternative method for composing pretrained models that directly uses language as the intermediate representation by which the modules exchange information with each other. It is both distinct from, and may be complementary to, other multimodal approaches such as joint training a single model over all modalities. SMs are perhaps most intuitively understood through examples, which are provided throughout the paper.</p><br>
                <img src="images/ego-perception.png">
            </div>


            <div class="section method">
                <h1><b>Highlight:</b> Zero-Shot Robot Perception and Planning</h1>
                <p>SMs can be used to enable robots to perform language-conditioned tasks. Our example system uses a VLM (open-vocabulary object detection with <a href="https://arxiv.org/abs/2104.13921">ViLD</a>) to describe the objects in the scene, feeds that description as context to a LM as a multi-step planner (e.g., <a href="https://say-can.github.io/">SayCan</a>, <a href="https://wenlong.page/language-planner/">Huang et al.</a>), that then generates the individual steps to be passed to a pretrained language-conditioned robot policy (e.g., models similar to <a href="https://cliport.github.io/">CLIPort</a> for open vocabulary pick-and-place). Chaining this system together expands the set of language-specified tasks beyond the original set of primitives trained by the policy, and enables applications involving human dialogue with the robot.</p><br>
                <p>
                    <video controls width="70%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/robots_demo.mp4" type="video/mp4">
                    </video>
                </p>
            </div>


            <div class="section ego-vqa">
                <h1><b>Highlight</b>: Zero-shot Open-ended Reasoning (i.e., Q&A, Forecasting) on Egocentric Video</h1>
                <p>Our example Socratic-Model-based system for egocentric perception can respond to a variety of open-ended text prompts -- from generating free-form answers to contextual reasoning questions, to forecasting future activities:</p><br>
                <!-- <img src="images/results-v4-website.png"> -->
                <p><video controls width="70%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/sm_2_ego_qa.mp4" type="video/mp4">
                </video>
                </p>
                <br>
                <p>This works by formulating <i>video understanding</i> as <i>reading comprehension</i>, i.e., re-framing “video Q&A” as a “short story Q&A” problem, which differs from common paradigms for video understanding that may involve supervising video-text models on labeled datasets or adversarial training. To this end, we first extract a set of “key moments” throughout the video (e.g., via importance sampling, or video/audio search based on the input query, discussed in Appendix). We then caption the key frames indexed by these moments, and recursively summarize them into a language-based record of events, which we term a language-based world-state history. This is then passed as context to an LM to perform various reasoning tasks via text completion such as Q&A, for which LMs have demonstrated strong zero-shot performance. Drawing analogies to 3D vision and robotics, the world-state history can be thought of as building an on-the-fly reconstruction of events in the observable world with language, rather than other representations, such as dynamically-updated 3D meshes or neural fields.</p><br>
                <p><video controls width="70%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/sm_3_ego_summary.mp4" type="video/mp4">
                </video>
                </p>
            </div>


            <div class="section method">
                <!-- <h1>Egocentric video example – watch the full source video on YouTube</h1> -->
                <br>
                <p><a href="https://www.youtube.com/channel/UCtJm2QGyeAFH1jcmQY6o-ZQ">Cody Wanner</a> was very nice to let us use his video content in our example demonstration for our research.  The examples above are all generated by using <a href="https://youtu.be/-UXKmqBPk1w">this video vlog</a> as input:</p>
                <br>

                <p align="center">
                <!-- pete note: i think most browsers wont autoplay if muted.  but I'd rather not let this play-->
                <!-- without sound, because the sound is too good (and we use it).  so let's not add the muted-->
                <!-- attribute i vote.-->
                <iframe width="630" height="354"
                src="https://www.youtube.com/embed/-UXKmqBPk1w?rel=0&amp;autoplay=1&mute=1">
                </iframe>
                </p>
                <br>
                
            </div>

            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->

            <div class="section method">
                <h1><b>Highlight:</b> Multimodal Assistive Dialogue</h1>
                <p>SMs can be adapted to engage in multimodal dialogue to assist people in every day tasks, such as cooking. Our example application helps the user search for a recipe, then guides them through it step by step. The system allows the user to navigate recipe steps with dialogue, provides ingredient replacements or advice (using LM priors), and searches for visual references (images/videos) on user request. This is a case study in (i) prompting a dialogue LM to produce key phrase tokens that elicit specific Socratic interactions (e.g., video search via a VLM to output visual data), and (ii) using a web crawler as an additional module (in the form of external API) engaged in Socratic discussion with other models to retrieve information online. Here is a live demo (with narration):</p><br>
                <p>
                    <video controls width="70%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/socratic-guided-recipes.mp4" type="video/mp4">
                    </video>
                </p>
            </div>

            <div class="section ego-vqa">
                <h1><b>Highlight</b>: Zero-Shot Image Captioning</h1>
                <p>We can also compose foundation models to caption images zero-shot, through closed-loop Socratic dialogue:</p>
                <br>
                <p><video controls width="75%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/sm_4_image_captioning_2.mp4" type="video/mp4">
                </video>
                </p>
                <br>
                <br>
                <p>Example captions below:</p>
                <br>
            </div>

            <div class="section image-captioning">
                <div class="gallery js-flickity"
                  data-flickity-options='{ "wrapAround": true, "draggable": false }'>

                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-02.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>People gather under a blossoming cherry tree, enjoying the beauty of nature together.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-03.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>At the outdoor market, you can find everything from plantains to Japanese bananas.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-02.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This image shows an inviting dining space with plenty of natural light.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-04.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A family celebrates a special occasion with ice cream and cake.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-05.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A wooden spoon and other kitchen utensils sit on a table in a restaurant kitchen.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-06.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A motorcycle lies abandoned in a sandy desert.</p>
                    </div>
                  </div>

                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-03.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This photo captures a person enjoying a meal at a restaurant. The spinach and nasturtium garnish on the plate makes for a beautiful and healthy meal.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-06.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This cartoon shows one person enjoying a relaxing bath with their scrub bird.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-04.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This photo was taken at a restaurant or pier. You can see the person enjoying their meal with a beautiful view of the water.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-01.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>The three people in this photo appear to be enjoying a close encounter with an elephant. This majestic creature looks like a gentle giant, and the handlers seem to have a great rapport with her. What a fun and unique experience for these tourists!</p>
                    </div>
                  </div>

                </div>
            </div>
            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->

            <div class="section ego-vqa">
                <h1><b>Highlight</b>: Zero-Shot Video-to-Text Retrieval</h1>
                <p>We also can compose mutiple models to perform zero-shot video-to-text retrieval -- this achieves state-of-the-art for zero-shot methods, nearing the gap with finetuned-on-the-dataset methods:</p>
                <br>
                <p><video controls width="75%" playsinline="" muted="" autoplay="" loop="">
                        <source src="images/sm_5_video_to_text.mp4" type="video/mp4">
                </video>
                </p>
            </div>



            <div style="clear: both;"></div>
            <div class="section team">
                <h1>Team</h1>
                <div class="people-profile">
                    <a href="https://andyzeng.github.io/"><img src="images/people/andy.jpg"><p>Andy Zeng</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/maria-attarian/"><img src="images/people/maria.jpeg"><p>Maria Attarian</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://brianichter.com/"><img src="images/people/brian.jpg"><p>Brian Ichter</p></a>
                </div>
                <div class="people-profile kchoro">
                    <a href="https://www.engineering.columbia.edu/krzysztof-choromanski"><img src="images/people/krzysztof.jpg"><p>Krzysztof Choromanski</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/almostsquare"><img src="images/people/adrian.png"><p>Adrian Wong</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/stefan-welker"><img src="images/people/stefan.jpg"><p>Stefan Welker</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://federicotombari.github.io/"><img src="images/people/federico.jpeg"><p>Federico Tombari</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/aveekly"><img src="images/people/aveek.jpeg"><p>Aveek Purohit</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://michaelryoo.com/"><img src="images/people/michael.jpg"><p>Michael Ryoo</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://vikas.sindhwani.org/"><img src="images/people/vikas.jpg"><p>Vikas Sindhwani</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/johnnychunglee"><img src="images/people/johnny.png"><p>Johnny Lee</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://vincent.vanhoucke.com/"><img src="images/people/vincent.png"><p>Vincent Vanhoucke</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://www.peteflorence.com/"><img src="images/people/pete.jpg"><p>Pete Florence</p></a>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="section teamlogo">
                <!-- <img src="images/logo.png"> -->
                <p>Robotics and Augmented Reality at Google</p>
            </div>
            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->






            <div class="section method">
                <h1>Method</h1>
                <!-- <p>This explainer is work in progress... &#9749;</p> -->
                <!-- <br> -->
                <img src="images/SM-discussion.png" style="width: 400px; float: right; margin-top: 0px;">
                <p>In this work we propose Socratic Models (SMs), a framework that uses structured dialogue between pre-existing foundation models, each of which can exhibit unique (but complementary) capabilities depending on the distributions of data on which they are trained. On various perceptual tasks, this work presents a case study of SMs with visual language models (VLMs, e.g., CLIP), large language models (LMs, e.g., GPT-3, RoBERTa), and audio language models (ALMs, e.g., Wav2CLIP, Speech2Text). From video search, to image captioning; from generating free-form answers to contextual reasoning questions, to forecasting future activities – SMs can provide meaningful results for complex tasks across classically challenging computer vision domains, without any model finetuning.</p>
                <br>
                <p>Examples of guided multi-model exchanges (Socratic Models) for an egocentric perception system: (i, left) parsing a natural language question into search entities (with LM) to be used to find the most relevant key moments in the video (with VLM); (ii, middle) describing each key frame by detecting places and objects (VLM), suggesting commonsense activities (LM), pruning the most likely activity (VLM), then generating a natural language summary (LM) of the SM interaction; (iii, right) concatenating key frame summaries into a language-based world-state history that an LM can use as context to answer the original question:</p>
                <br>
                <img src="images/SM-system.png">
                <br>
                <br>
                <p>SMs can interface with the user through dialogue and perform a variety of tasks (formulated as Q&A) with egocentric video: sorting reasoning questions by their output modalities e.g., text-base responses, images from visual search, video snippets from audio search. Depending on the modality, each question can pass through a different sequence of Socratic interactions between the LM, VLM, and ALM:</p>
                <br>
                <img src="images/ego-vqa-system.png">
                <br>
                <br>
                <!-- <img src="images/ego-vqa-results.png"> -->

                <!-- <img src="images/ego-perception.png">
                <img src="images/ego-vqa-system.png">
                <img src="images/ego-vqa-system.png">
                <p></p> -->




            </div>






            <div id="code">
                <div class="section abstract half colab">
                    <h2>Code</h2>
                    <p>We plan to release prototypes in the form of self-contained colabs. They will be added to <a href="https://github.com/google-research/google-research/tree/master/socraticmodels">this repository</a> and linked here:<br>
                        <!-- <br>&nbsp;&nbsp;•&nbsp;&nbsp;Image Captioning&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://colab.research.google.com/drive/1KOlc9nN0NJ5GAif_dmuOqsRqlZycoIrc?usp=sharing"><img src="images/colabicon.png" style="height: 20px"></a> -->
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Image Captioning - <a href="https://colab.research.google.com/drive/1KOlc9nN0NJ5GAif_dmuOqsRqlZycoIrc?usp=sharing">Open in Colab</a>
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Video Understanding (MSR-VTT) - <a href="https://colab.research.google.com/drive/1mSKNbO1rK6CTJoJ5HifE5SftADAQzE70?usp=sharing">Open in Colab</a> 
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Egocentric Video Q&A - Coming soon
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Robot Perception & Planning - <a href="https://colab.research.google.com/drive/1jAyhumd7DTxJB2oZufob9crVxETAEKbV?usp=sharing">Open in Colab</a>
                    </p>
                    <br>
                </div>
            </div>
            <div class="section bibtex half">
                <h2>Citation</h2>
                <div class="bib">@article{zeng2022socraticmodels,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author={Andy Zeng and Maria Attarian and Brian Ichter and Krzysztof Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},<br>
&nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year={2022}<br>
}</div>
            </div>


            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->


            <div class="section method">
                <h1>Acknowledgements</h1>
                <p>We thank Debidatta Dwibedi, Matthew O’Kelly, and Kevin Zakka for excellent feedback on improving this manuscript, Anelia Angelova, Jean-Jacques Slotine, Jonathan Tompson, Shuran Song, for fruitful technical discussions, Kan Huang for applications support, Ahmed Omran, Aren Jensen, Malcolm Slaney, Karolis Misiunas for advice on audio models, and Cody Wanner for YouTube videos.</p>
                <br>
            </div>
            <br><br><br><br><br><br><br><br><br><br>
            <div class="divider"></div>
        </div>
    </body>
</html>