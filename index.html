<!DOCTYPE HTML>
<html>
    <head>
        <title>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="flickity.min.css">
        <script src="flickity.pkgd.min.js"></script>
        <link rel="stylesheet" href="style.css" />
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header">
            </header>
            <!-- style="padding-bottom:1em" -->
            <div id="profile">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc">
                    <div id="profile-name">Socratic Models</div>
                    <div id="profile-subname">Composing Zero-Shot Multimodal Reasoning with Language</div>
                    <!-- <div id="profile-authors">
                        Andy Zeng&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Adrian Wong&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Stefan Welker&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Krzysztof Choromanski&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Federico Tombari<br>
                        Aveek Purohit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Michael Ryoo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Vikas Sindhwani&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Johnny Lee&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Vincent Vanhoucke&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        Pete Florence<br>Google</div> -->

                    <!-- <p>
                        <b>Abstract</b>. Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass object(s) or an end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input -- which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world.
                    </p> -->
                </div>
                <div id="profile-icons-wrapper">
                    <img src="images/internet-venn.png" style="height: 0px; margin: 0px; padding: 0px;"> <!-- thumbnail -->
                    <div class="profile-icon"><a href=""><img src="images/paper-icon.png"></a><p>Paper</p></div>
                    <div class="profile-icon"><a href="#code"><img src="images/github.png"></a><p>Code</p></div>
                </div>
                <div style="clear: both;"></div>
            </div>

            <div class="section abstract">
                <h1>Abstract</h1>
                <img src="images/internet-venn.png" style="height: auto; width: 500px">
                <p>Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in which new multimodal tasks are formulated as a guided language-based exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating free-form answers to contextual questions about egocentric video, by formulating video Q&A as short story Q&A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zero-shot to capture new multimodal functionalities, without domain-specific data collection.</p>
                <br>
            </div>

            <div class="section image-captioning">
                <h1>Image Captioning</h1>
                <div class="gallery js-flickity"
                  data-flickity-options='{ "wrapAround": true, "draggable": false }'>

                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-02.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>People gather under a blossoming cherry tree, enjoying the beauty of nature together.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-03.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>At the outdoor market, you can find everything from plantains to Japanese bananas.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-02.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This image shows an inviting dining space with plenty of natural light.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-04.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A family celebrates a special occasion with ice cream and cake.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-05.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A wooden spoon and other kitchen utensils sit on a table in a restaurant kitchen.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-06.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>A motorcycle lies abandoned in a sandy desert.</p>
                    </div>
                  </div>

                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-03.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This photo captures a person enjoying a meal at a restaurant. The spinach and nasturtium garnish on the plate makes for a beautiful and healthy meal.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-06.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This cartoon shows one person enjoying a relaxing bath with their scrub bird.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/cc-04.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>This photo was taken at a restaurant or pier. You can see the person enjoying their meal with a beautiful view of the water.</p>
                    </div>
                  </div>
                  <div class="gallery-cell">
                    <div class="gallery-cell-image-wrapper">
                      <img src="images/results/coco-01.jpeg">
                    </div>
                    <div class="gallery-cell-caption-wrapper">
                        <p>The three people in this photo appear to be enjoying a close encounter with an elephant. This majestic creature looks like a gentle giant, and the handlers seem to have a great rapport with her. What a fun and unique experience for these tourists!</p>
                    </div>
                  </div>

                </div>
            </div>
            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->
            <div class="section ego-vqa">
                <h1>Egocentric Video Q&A</h1>
                <p>More illustrations and examples coming soon</p>
                <!-- <img src="images/SM-system.png"> -->
                <img src="images/ego-perception.png">
            </div>
            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->

            <div style="clear: both;"></div>
            <div class="section team">
                <h1>Team</h1>
                <div class="people-profile">
                    <a href="https://andyzeng.github.io/"><img src="images/people/andy.jpg"><p>Andy Zeng</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/almostsquare"><img src="images/people/adrian.png"><p>Adrian Wong</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/stefan-welker"><img src="images/people/stefan.jpg"><p>Stefan Welker</p></a>
                </div>
                <div class="people-profile kchoro">
                    <a href="https://www.engineering.columbia.edu/krzysztof-choromanski"><img src="images/people/krzysztof.jpg"><p>Krzysztof Choromanski</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://federicotombari.github.io/"><img src="images/people/federico.jpeg"><p>Federico Tombari</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/aveekly"><img src="images/people/aveek.jpeg"><p>Aveek Purohit</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://michaelryoo.com/"><img src="images/people/michael.jpg"><p>Michael Ryoo</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://vikas.sindhwani.org/"><img src="images/people/vikas.jpg"><p>Vikas Sindhwani</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://www.linkedin.com/in/johnnychunglee"><img src="images/people/johnny.png"><p>Johnny Lee</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://vincent.vanhoucke.com/"><img src="images/people/vincent.png"><p>Vincent Vanhoucke</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://www.peteflorence.com/"><img src="images/people/pete.jpg"><p>Pete Florence</p></a>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="section teamlogo">
                <!-- <img src="images/logo.png"> -->
                <p>Robotics and Augmented Reality at Google</p>
            </div>
            <div style="clear: both;"></div>
            <!-- <div class="divider"></div> -->






            <div class="section method">
                <h1>Method</h1>
                <p>This explainer is work in progress... &#9749;</p>
                <br>
                <img src="images/SM-discussion.png" style="width: 400px; float: right; margin-top: 0px;">
                <p>In this work we propose Socratic Models (SMs), a framework that uses structured dialogue between pre-existing foundation models, each of which can exhibit unique (but complementary) capabilities depending on the distributions of data on which they are trained. On various perceptual tasks, this work presents a case study of SMs with visual language models (VLMs, e.g., CLIP), large language models (LMs, e.g., GPT-3, RoBERTa), and audio language models (ALMs, e.g., Wav2CLIP, Speech2Text). From video search, to image captioning; from generating free-form answers to contextual reasoning questions, to forecasting future activities – SMs can provide meaningful results for complex tasks across classically challenging computer vision domains, without any model finetuning.</p>
                <br>
                <p>Examples of guided multi-model exchanges (Socratic Models) for an egocentric perception system: (i, left) parsing a natural language question into search entities (with LM) to be used to find the most relevant key moments in the video (with VLM); (ii, middle) describing each key frame by detecting places and objects (VLM), suggesting commonsense activities (LM), pruning the most likely activity (VLM), then generating a natural language summary (LM) of the SM interaction; (iii, right) concatenating key frame summaries into a language-based world-state history that an LM can use as context to answer the original question:</p>
                <img src="images/SM-system.png">
                <br>
                <br>
                <p>SMs can interface with the user through dialogue and perform a variety of tasks (formulated as Q&A) with egocentric video: sorting reasoning questions by their output modalities e.g., text-base responses, images from visual search, video snippets from audio search. Depending on the modality, each question can pass through a different sequence of Socratic interactions between the LM, VLM, and ALM:</p>
                <img src="images/ego-vqa-system.png">
                <br>
                <br>
                <!-- <img src="images/ego-vqa-results.png"> -->

                <!-- <img src="images/ego-perception.png">
                <img src="images/ego-vqa-system.png">
                <img src="images/ego-vqa-system.png">
                <p></p> -->




            </div>









            <div id="code">
                <div class="section abstract half colab">
                    <h1>Code</h1>
                    <p>We plan to release prototypes in the form of self-contained colabs. They will be added to <a href="https://github.com/google-research/google-research/tree/master/socraticmodels">this repository</a> and linked here:<br>
                        <!-- <br>&nbsp;&nbsp;•&nbsp;&nbsp;Image Captioning&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://colab.research.google.com/drive/1KOlc9nN0NJ5GAif_dmuOqsRqlZycoIrc?usp=sharing"><img src="images/colabicon.png" style="height: 20px"></a> -->
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Image Captioning - <a href="https://colab.research.google.com/drive/1KOlc9nN0NJ5GAif_dmuOqsRqlZycoIrc?usp=sharing">Open in Colab</a>
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Egocentric Video Q&A - Coming soon
                        <br>&nbsp;&nbsp;•&nbsp;&nbsp;Video Understanding (MSR-VTT) - Coming soon 
                    </p>
                    <br>
                </div>
            </div>
            <div class="section bibtex half">
                <h1>Citation</h1>
                <div class="code">@article{zeng2022socraticmodels,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author={Andy Zeng and Adrian Wong and Stefan Welker and Krzysztof Choromanski and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},<br>
&nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year={2022}<br>
}</div>
            </div>


            <div class="divider"></div>


            <div class="section abstract">
                <h1>Egocentric video example -- watch the full source video on YouTube</h1>

                <p align="center">
                <!-- pete note: i think most browsers wont autoplay if muted.  but I'd rather not let this play-->
                <!-- without sound, because the sound is too good (and we use it).  so let's not add the muted-->
                <!-- attribute i vote.-->
                <iframe width="900" height="506"
                src="https://www.youtube.com/embed/-UXKmqBPk1w?autoplay=1">
                </iframe>
                </p>
                <br>
                <p><a href="https://www.youtube.com/channel/UCtJm2QGyeAFH1jcmQY6o-ZQ">Cody Wanner</a> was very nice to let us use his great video content in our example demonstration for our research.  If you also appreciate Cody for supporting researchers... as they say on YouTube... please consider Liking & Subscribing? :)</p>
                <br>
            </div>


            <div class="section abstract">
                <h1>Acknowledgements</h1>
                <p>We thank Debidatta Dwibedi and Matthew O’Kelly for excellent feedback on improving this manuscript, Anelia Angelova, Jean-Jacques Slotine, Jonathan Tompson, Maria Attarian, Shuran Song, for fruitful technical discussions, Kan Huang for applications support, Ahmed Omran, Aren Jensen, Malcolm Slaney, Karolis Misiunas for advice on audio models, and Cody Wanner for YouTube videos.</p>
                <br>
            </div>
            <br><br><br><br><br><br><br><br><br><br>
            <div class="divider"></div>
        </div>
    </body>
</html>